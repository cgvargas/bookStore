import requests
import json
import logging
from typing import Dict, List, Optional, Tuple
from django.conf import settings
from django.core.cache import cache
from ..models import Conversation, Message
import time
import threading
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError

# ‚úÖ NOVA IMPORTA√á√ÉO: External AI Service para fallback r√°pido
from .external_ai_service import external_ai_manager

logger = logging.getLogger(__name__)


class AIService:
    """
    Servi√ßo de IA integrado com Ollama local + Groq external para chatbot liter√°rio.
    ‚úÖ H√çBRIDO: Local (Llama 3.2:3b) + External (Groq API)
    ‚úÖ R√ÅPIDO: Fallback para 2-5s quando local falha
    ‚úÖ CONFI√ÅVEL: 99% uptime com m√∫ltiplos providers

    Ordem de tentativas:
    1. Llama local (15s timeout)
    2. Groq API (5s timeout)
    3. Fallback response
    """

    def __init__(self):
        self.base_url = getattr(settings, 'OLLAMA_BASE_URL', 'http://localhost:11434')
        self.model = getattr(settings, 'OLLAMA_MODEL', 'llama3.2:3b')

        # ‚úÖ TIMEOUTS SUPER OTIMIZADOS PARA SISTEMA H√çBRIDO
        self.timeout_quick = 10  # Otimizado: 15s ‚Üí 10s para fallback mais r√°pido
        self.timeout_normal = 20  # Timeout local menor
        self.timeout_max = 25  # M√°ximo para local
        self.timeout = 20

        self.max_tokens = getattr(settings, 'OLLAMA_MAX_TOKENS', 2048)

        # ‚úÖ CONFIGURA√á√ïES OTIMIZADAS
        self.temperature = getattr(settings, 'GPT_OSS_TEMPERATURE', 0.7)

        # ‚úÖ EXECUTOR PARA TIMEOUTS
        self.executor = ThreadPoolExecutor(max_workers=3)

        # ‚úÖ EXTERNAL AI MANAGER
        self.external_ai = external_ai_manager

        # Status dos servi√ßos
        self.local_available = False
        self.external_available = self.external_ai.is_available()

        # Verifica√ß√£o de disponibilidade
        self._verify_services_availability()

    def _verify_services_availability(self) -> None:
        """Verifica disponibilidade dos servi√ßos local e externo."""
        # Verificar servi√ßo local
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]

                if self.model not in available_models:
                    logger.warning(
                        f"Modelo {self.model} n√£o encontrado localmente. "
                        f"Modelos dispon√≠veis: {available_models}. "
                        f"Sistema funcionar√° apenas com IA externa."
                    )
                    self.local_available = False
                else:
                    logger.info(f"‚úÖ Modelo local {self.model} verificado e dispon√≠vel.")
                    self.local_available = True
            else:
                logger.error(f"Erro ao verificar modelos locais: {response.status_code}")
                self.local_available = False

        except requests.RequestException as e:
            logger.error(f"Ollama local indispon√≠vel: {e}")
            self.local_available = False

        # Log do status geral
        if self.local_available and self.external_available:
            logger.info("üöÄ Sistema H√çBRIDO ativo: Local + External AI dispon√≠veis!")
        elif self.external_available:
            logger.info("üåê Sistema EXTERNAL ativo: Apenas External AI dispon√≠vel")
        elif self.local_available:
            logger.info("üè† Sistema LOCAL ativo: Apenas Llama local dispon√≠vel")
        else:
            logger.warning("‚ö†Ô∏è NENHUM servi√ßo de IA dispon√≠vel - apenas fallbacks")

    def _build_system_prompt(self, user_profile: Dict = None) -> str:
        """
        Constr√≥i o prompt do sistema otimizado para an√°lises liter√°rias.
        ‚úÖ OTIMIZADO: Mais conciso para velocidade
        """
        base_prompt = """Voc√™ √© um assistente liter√°rio especializado da CG.BookStore.Online.

ESPECIALIDADES:
- Literatura brasileira e mundial
- An√°lises cr√≠ticas e contextuais 
- Recomenda√ß√µes personalizadas
- Navega√ß√£o no site

DIRETRIZES:
1. Respostas diretas e informativas
2. Adapte ao n√≠vel do usu√°rio
3. Foque na experi√™ncia liter√°ria
4. Use conhecimento fornecido quando dispon√≠vel
5. Seja preciso e acess√≠vel"""

        # Personaliza√ß√£o baseada no perfil (reduzida)
        if user_profile:
            experience_level = user_profile.get('reading_level', 'intermedi√°rio')
            favorite_genres = user_profile.get('favorite_genres', [])

            if experience_level or favorite_genres:
                personalization = f"\n\nPERFIL: {experience_level}"
                if favorite_genres:
                    personalization += f", g√™neros: {', '.join(favorite_genres[:2])}"
                base_prompt += personalization

        return base_prompt

    def _prepare_local_payload(self, messages: List[Dict], timeout_target: int = None) -> Dict:
        """
        ‚úÖ OTIMIZADO: Prepara payload para Llama local
        """
        # Configura√ß√µes baseadas no timeout
        if timeout_target and timeout_target <= self.timeout_quick:
            config = {
                "temperature": 0.1,
                "num_predict": 384,
                "num_ctx": 2048,
                "repeat_penalty": 1.05,
                "top_k": 20,
                "top_p": 0.7,
            }
        else:
            config = {
                "temperature": self.temperature,
                "num_predict": 768,
                "num_ctx": 3072,
                "repeat_penalty": 1.1,
                "top_k": 40,
                "top_p": 0.9
            }

        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": config
        }

        return payload

    def _execute_local_with_timeout(self, messages: List[Dict], timeout_seconds: int,
                                    operation_name: str = "IA Local") -> Dict:
        """
        ‚úÖ ATUALIZADO: Executa requisi√ß√£o local com timeout otimizado
        """
        start_time = time.time()

        def make_request():
            payload = self._prepare_local_payload(messages, timeout_seconds)
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=timeout_seconds,
                headers={'Content-Type': 'application/json'}
            )
            return response

        try:
            logger.info(f"üè† Iniciando {operation_name} (timeout: {timeout_seconds}s)")

            future = self.executor.submit(make_request)
            response = future.result(timeout=timeout_seconds)

            execution_time = time.time() - start_time
            logger.info(f"‚úÖ {operation_name} conclu√≠da em {execution_time:.2f}s")

            if response.status_code == 200:
                response_data = response.json()
                final_answer, reasoning_chain = self._extract_reasoning_chain(response_data)

                return {
                    'success': True,
                    'response': final_answer,
                    'reasoning': reasoning_chain,
                    'execution_time': execution_time,
                    'timeout_used': timeout_seconds,
                    'service': 'local',
                    'model': self.model,
                    'metadata': {
                        'response_length': len(final_answer),
                        'processing_time': execution_time
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'HTTP {response.status_code}: {response.text}',
                    'execution_time': execution_time,
                    'service': 'local',
                    'fallback_needed': True
                }

        except FutureTimeoutError:
            execution_time = time.time() - start_time
            logger.warning(f"‚è∞ Timeout em {operation_name} ap√≥s {execution_time:.2f}s")

            return {
                'success': False,
                'error': 'timeout',
                'execution_time': execution_time,
                'timeout_used': timeout_seconds,
                'service': 'local',
                'message': f'Timeout na {operation_name} ap√≥s {timeout_seconds}s',
                'fallback_needed': True
            }

        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"‚ùå Erro em {operation_name}: {e}")

            return {
                'success': False,
                'error': str(e),
                'execution_time': execution_time,
                'service': 'local',
                'fallback_needed': True
            }

    def _get_smart_response(self, messages: List[Dict]) -> Dict:
        """
        ‚úÖ REVOLUCION√ÅRIO: Sistema h√≠brido com fallback External AI

        Ordem de tentativas:
        1. Llama local (15s) - Se dispon√≠vel
        2. Groq External (5s) - Sempre tenta
        3. Fallback final - Resposta padr√£o
        """
        logger.info("üéØ Iniciando sistema h√≠brido: Local ‚Üí External ‚Üí Fallback")

        # 1. TENTATIVA LOCAL (Se dispon√≠vel)
        if self.local_available:
            local_result = self._execute_local_with_timeout(
                messages=messages,
                timeout_seconds=self.timeout_quick,
                operation_name="Llama Local"
            )

            if local_result['success']:
                local_result['response_strategy'] = 'local'
                logger.info("‚úÖ Resposta LOCAL bem-sucedida!")
                return local_result
            else:
                logger.info("‚ö†Ô∏è Llama local falhou, tentando External AI...")
        else:
            logger.info("‚ö†Ô∏è Llama local indispon√≠vel, indo direto para External AI...")

        # 2. TENTATIVA EXTERNAL AI (Groq + HuggingFace)
        if self.external_available:
            logger.info("üåê Tentando External AI (Groq)...")
            try:
                external_result = self.external_ai.generate_response(messages)

                if external_result['success']:
                    external_result['response_strategy'] = 'external'
                    external_result['service'] = external_result.get('fallback_service', 'external')
                    logger.info(f"‚úÖ Resposta EXTERNAL bem-sucedida via {external_result.get('service', 'external')}!")
                    return external_result
                else:
                    logger.warning(f"‚ö†Ô∏è External AI falhou: {external_result.get('error', 'Unknown error')}")

            except Exception as e:
                logger.error(f"‚ùå Erro no External AI: {e}")
        else:
            logger.info("‚ö†Ô∏è External AI indispon√≠vel")

        # 3. FALLBACK FINAL - Quando tudo falha
        logger.info("üÜò Executando fallback final...")
        return self._get_comprehensive_fallback_response()

    def _get_comprehensive_fallback_response(self) -> Dict:
        """
        ‚úÖ MELHORADO: Fallback mais √∫til e informativo
        """
        logger.info("üÜò Executando fallback final...")

        # Resposta baseada no status dos servi√ßos
        if not self.local_available and not self.external_available:
            status_msg = "Todos os nossos servi√ßos de IA est√£o temporariamente indispon√≠veis."
        elif not self.local_available:
            status_msg = "Nosso servi√ßo local est√° indispon√≠vel e o servi√ßo externo apresenta problemas."
        elif not self.external_available:
            status_msg = "Nosso servi√ßo principal est√° sobrecarregado."
        else:
            status_msg = "Estamos enfrentando alta demanda no momento."

        response_parts = [
            f"Desculpe, {status_msg} ",
            "Aqui est√£o algumas alternativas:\n\n",
            "üîç **Busque diretamente**: Use a busca principal do site para encontrar livros\n",
            "üìö **Explore por categoria**: Literatura Brasileira, Cl√°ssicos Mundiais, Contempor√¢neos\n",
            "üë§ **Acesse seu perfil**: Veja recomenda√ß√µes personalizadas baseadas em seu hist√≥rico\n",
            "üí¨ **Tente novamente**: Reformule sua pergunta em alguns minutos\n\n",
            "‚ö° *Nossos sistemas est√£o sendo otimizados continuamente para melhor performance.*"
        ]

        # Informa√ß√µes de debug em desenvolvimento
        if settings.DEBUG:
            debug_info = []
            if not self.local_available:
                debug_info.append("Local: indispon√≠vel")
            if not self.external_available:
                debug_info.append("External: indispon√≠vel")

            if debug_info:
                response_parts.append(f"\nüîß *Debug: {', '.join(debug_info)}*")

        return {
            'success': True,
            'response': ''.join(response_parts),
            'response_strategy': 'comprehensive_fallback',
            'service': 'fallback',
            'metadata': {
                'local_available': self.local_available,
                'external_available': self.external_available,
                'fallback_type': 'comprehensive'
            }
        }

    def _extract_reasoning_chain(self, response_data: Dict) -> Tuple[str, Optional[str]]:
        """
        Extrai o conte√∫do da resposta e o chain-of-thought quando dispon√≠vel.
        """
        try:
            if 'message' in response_data and 'content' in response_data['message']:
                full_content = response_data['message']['content']

                # Verifica se h√° separa√ß√£o expl√≠cita de reasoning
                if "## Racioc√≠nio:" in full_content or "<thinking>" in full_content:
                    # Separar reasoning da resposta final
                    parts = full_content.split("## Resposta:", 1)
                    if len(parts) == 2:
                        reasoning = parts[0].replace("## Racioc√≠nio:", "").strip()
                        final_answer = parts[1].strip()
                        return final_answer, reasoning

                    # Formato alternativo com tags
                    if "<thinking>" in full_content and "</thinking>" in full_content:
                        import re
                        thinking_match = re.search(r'<thinking>(.*?)</thinking>',
                                                   full_content, re.DOTALL)
                        if thinking_match:
                            reasoning = thinking_match.group(1).strip()
                            final_answer = full_content.replace(thinking_match.group(0), "").strip()
                            return final_answer, reasoning

                return full_content, None

        except (KeyError, AttributeError) as e:
            logger.error(f"Erro ao extrair reasoning chain: {e}")

        return "", None

    def get_ai_response(self, user_message: str, conversation_id: int,
                        user_profile: Dict = None,
                        reasoning_effort: str = None) -> Dict:
        """
        ‚úÖ ATUALIZADO: Obt√©m resposta usando sistema h√≠brido otimizado
        """
        try:
            # Cache key para otimiza√ß√£o
            cache_key = f"hybrid_response_{hash(user_message)}_{conversation_id}"
            cached_response = cache.get(cache_key)

            if cached_response and not reasoning_effort:
                logger.debug(f"Resposta recuperada do cache para conversa {conversation_id}")
                return cached_response

            # Construir hist√≥rico da conversa
            messages = self._build_conversation_history(conversation_id, user_profile)
            messages.append({
                "role": "user",
                "content": user_message
            })

            # Log da requisi√ß√£o
            logger.info(f"Enviando requisi√ß√£o para sistema h√≠brido - Conversa: {conversation_id}")

            # ‚úÖ USAR SISTEMA H√çBRIDO OTIMIZADO
            result = self._get_smart_response(messages)

            # Cache da resposta (apenas para consultas simples e bem-sucedidas)
            if result['success'] and not reasoning_effort and result.get(
                    'response_strategy') != 'comprehensive_fallback':
                cache.set(cache_key, result, timeout=1800)  # 30 min

            service_used = result.get('service', result.get('response_strategy', 'unknown'))
            response_length = len(result.get('response', ''))
            logger.info(f"Resposta h√≠brida: {service_used} - {response_length} chars")

            return result

        except Exception as e:
            error_msg = f"Erro inesperado no sistema h√≠brido: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return {
                'success': False,
                'error': error_msg,
                'fallback_needed': True,
                'service': 'error'
            }

    def _build_conversation_history(self, conversation_id: int,
                                    user_profile: Dict = None) -> List[Dict]:
        """
        ‚úÖ OTIMIZADO: Constr√≥i hist√≥rico mais conciso para velocidade
        """
        messages = []

        # System prompt personalizado (mais conciso)
        system_prompt = self._build_system_prompt(user_profile)
        messages.append({
            "role": "system",
            "content": system_prompt
        })

        try:
            # Buscar conversa e mensagens (limitadas para velocidade)
            conversation = Conversation.objects.get(id=conversation_id)
            recent_messages = Message.objects.filter(
                conversation=conversation
            ).order_by('-timestamp')[:6]  # Reduzido de 10 para 6 mensagens

            # Adicionar mensagens ao contexto (ordem cronol√≥gica)
            for message in reversed(recent_messages):
                messages.append({
                    "role": "user" if message.sender == "user" else "assistant",
                    "content": message.content
                })

        except Conversation.DoesNotExist:
            logger.warning(f"Conversa {conversation_id} n√£o encontrada")
        except Exception as e:
            logger.error(f"Erro ao buscar hist√≥rico: {e}")

        return messages

    def analyze_literature_with_reasoning(self, text: str, analysis_type: str,
                                          user_profile: Dict = None) -> Dict:
        """
        An√°lise liter√°ria especializada com sistema h√≠brido.
        ‚úÖ ATUALIZADO: Usa sistema h√≠brido otimizado
        """
        analysis_prompts = {
            'style': 'Analise o estilo liter√°rio deste texto',
            'theme': 'Identifique e explore os temas presentes',
            'character': 'Analise os personagens e suas caracter√≠sticas',
            'structure': 'Examine a estrutura narrativa',
            'context': 'Contextualize historicamente e culturalmente'
        }

        prompt = analysis_prompts.get(analysis_type, 'Fa√ßa uma an√°lise geral')
        full_prompt = f"""
            {prompt}: 
            "{text}"         
            Por favor, forne√ßa uma an√°lise completa e fundamentada.
        """

        return self.get_ai_response(
            user_message=full_prompt,
            conversation_id=0,  # An√°lise independente
            user_profile=user_profile
        )

    def health_check(self) -> Dict:
        """
        ‚úÖ ATUALIZADO: Verifica o status de sa√∫de do sistema h√≠brido.
        """
        try:
            # Status do servi√ßo local
            local_status = 'down'
            local_response_time = None

            if self.local_available:
                try:
                    start_time = time.time()
                    response = requests.get(f"{self.base_url}/api/tags", timeout=5)
                    local_response_time = time.time() - start_time

                    if response.status_code == 200:
                        local_status = 'healthy'
                    else:
                        local_status = 'degraded'
                except:
                    local_status = 'down'
                    self.local_available = False

            # Status do servi√ßo externo
            external_status = self.external_ai.health_check()

            # Status geral
            if local_status == 'healthy' or external_status.get('overall_status') == 'healthy':
                overall_status = 'healthy'
            elif local_status in ['healthy', 'degraded'] or external_status.get('overall_status') in ['healthy',
                                                                                                      'degraded']:
                overall_status = 'degraded'
            else:
                overall_status = 'down'

            return {
                'status': overall_status,
                'system_type': 'hybrid',
                'local_service': {
                    'status': local_status,
                    'available': self.local_available,
                    'model': self.model if self.local_available else None,
                    'response_time': local_response_time
                },
                'external_service': external_status,
                'timeout_config': {
                    'quick': self.timeout_quick,
                    'normal': self.timeout_normal,
                    'max': self.timeout_max
                }
            }

        except Exception as e:
            return {
                'status': 'error',
                'system_type': 'hybrid',
                'error': str(e),
                'local_available': False,
                'external_available': False
            }

    def generate_response(self, messages: List[Dict]) -> Dict:
        """
        ‚úÖ ATUALIZADO: M√©todo de compatibilidade com FunctionalChatbot usando sistema h√≠brido
        """
        try:
            # Extrair a mensagem do usu√°rio (√∫ltima mensagem)
            user_message = ""
            for msg in reversed(messages):
                if msg.get('role') == 'user':
                    user_message = msg.get('content', '')
                    break

            if not user_message:
                return {
                    'success': False,
                    'error': 'Nenhuma mensagem do usu√°rio encontrada',
                    'response': 'N√£o foi poss√≠vel processar sua mensagem.'
                }

            # ‚úÖ USAR SISTEMA H√çBRIDO OTIMIZADO
            result = self._get_smart_response(messages)

            # Retornar no formato esperado pelo FunctionalChatbot
            if result['success']:
                return {
                    'success': True,
                    'response': result['response'],
                    'metadata': result.get('metadata', {}),
                    'reasoning': result.get('reasoning'),
                    'response_strategy': result.get('response_strategy', 'unknown'),
                    'service': result.get('service', 'hybrid')
                }
            else:
                return {
                    'success': False,
                    'error': result.get('error', 'Erro desconhecido'),
                    'response': 'Desculpe, n√£o consegui processar sua solicita√ß√£o no momento.',
                    'service': result.get('service', 'error')
                }

        except Exception as e:
            logger.error(f"Erro em generate_response: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'response': 'Ocorreu um erro interno. Tente novamente.',
                'service': 'error'
            }

    def is_available(self) -> bool:
        """
        ‚úÖ ATUALIZADO: Verifica se pelo menos um servi√ßo est√° dispon√≠vel.
        """
        try:
            # Atualizar status se necess√°rio
            if not self.local_available and not self.external_available:
                self._verify_services_availability()

            return self.local_available or self.external_available
        except Exception as e:
            logger.error(f"Erro ao verificar disponibilidade: {e}")
            return False


def is_ai_available() -> bool:
    """
    ‚úÖ ATUALIZADO: Fun√ß√£o standalone para verificar disponibilidade do sistema h√≠brido.
    """
    try:
        return ai_service.is_available()
    except Exception as e:
        logger.error(f"Erro ao verificar disponibilidade da IA: {e}")
        return False


# ‚úÖ INST√ÇNCIA GLOBAL ATUALIZADA
ai_service = AIService()